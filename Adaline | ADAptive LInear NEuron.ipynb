{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Linear Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaline was published, only a few years after Frank Rosenblatt's perceptron algorithm, by Bernard Widrow and his doctoral student Tedd Hoff, and can be considered as an improvement on the latter (B. Widrow et al. Adaptive \"Adaline\" neuron using chemical \"memistors\". Number Technical Report 1553-2. Stanford Electron. Labs. Stanford, CA, October 1960). The Adaline algorithm is particularly interesting because it illustrates the key concept of defining and minimizing cost functions, which will lay the groundwork for understanding more advanced machine learning algorithms for classification, such as logistic regression and support vector machines, as well as regression models.\n",
    "Raschka, Sebastian (2015-09-23). Python Machine Learning (p. 33). Packt Publishing. Kindle Edition. \n",
    "\n",
    "### The key difference between the Adaline rule (also known as the Widrow-Hoff rule) and Rosenblatt's perceptron is that the weights are updated based on a linear activation function rather than a unit step function like in the perceptron.\n",
    "Raschka, Sebastian (2015-09-23). Python Machine Learning (p. 33). Packt Publishing. Kindle Edition.\n",
    "\n",
    "### One of the key ingredients of supervised machine learning algorithms is to define an objective function that is to be optimized during the learning process. This objective function is often a cost function that we want to minimize. In the case of Adaline, we can define the cost function to learn the weights as the Sum of Squared Errors (SSE) between the calculated outcome and the true class label.\n",
    "\n",
    "Raschka, Sebastian (2015-09-23). Python Machine Learning (p. 34). Packt Publishing. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing an Adaptive Linear Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the perceptron rule and Adaline are similar enough to replicate the perceptron implementation with a\n",
    "# change to the fit method for the gradient descent minimizing of the cost function weight update\n",
    "\n",
    "class AdalineGD(object):\n",
    "    '''\n",
    "    ADAptive LInear NEuron classifier\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Passes over the training dataset\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting\n",
    "    errors_ : list\n",
    "        Number of misclassification in every epoch\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def fit(self, X, y):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
