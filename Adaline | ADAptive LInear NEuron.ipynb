{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Linear Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaline was published, only a few years after Frank Rosenblatt's perceptron algorithm, by Bernard Widrow and his doctoral student Tedd Hoff, and can be considered as an improvement on the latter (B. Widrow et al. Adaptive \"Adaline\" neuron using chemical \"memistors\". Number Technical Report 1553-2. Stanford Electron. Labs. Stanford, CA, October 1960). The Adaline algorithm is particularly interesting because it illustrates the key concept of defining and minimizing cost functions, which will lay the groundwork for understanding more advanced machine learning algorithms for classification, such as logistic regression and support vector machines, as well as regression models.\n",
    "Raschka, Sebastian (2015-09-23). Python Machine Learning (p. 33). Packt Publishing. Kindle Edition. \n",
    "\n",
    "### The key difference between the Adaline rule (also known as the Widrow-Hoff rule) and Rosenblatt's perceptron is that the weights are updated based on a linear activation function rather than a unit step function like in the perceptron.\n",
    "Raschka, Sebastian (2015-09-23). Python Machine Learning (p. 33). Packt Publishing. Kindle Edition.\n",
    "\n",
    "### One of the key ingredients of supervised machine learning algorithms is to define an objective function that is to be optimized during the learning process. This objective function is often a cost function that we want to minimize. In the case of Adaline, we can define the cost function to learn the weights as the Sum of Squared Errors (SSE) between the calculated outcome and the true class label.\n",
    "\n",
    "Raschka, Sebastian (2015-09-23). Python Machine Learning (p. 34). Packt Publishing. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing an Adaptive Linear Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the perceptron rule and Adaline are similar enough to replicate the perceptron implementation with a\n",
    "# change to the fit method for the gradient descent minimizing of the cost function weight update\n",
    "\n",
    "class AdalineGD(object):\n",
    "    '''\n",
    "    ADAptive LInear NEuron classifier\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Passes over the training dataset\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting\n",
    "    errors_ : list\n",
    "        Number of misclassification in every epoch\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Fit training data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : [array-like], shape = [n_samples, n_features]\n",
    "            Training vectors,\n",
    "            where n_samples is the number of samples and\n",
    "            n_features is the number of features\n",
    "            \n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        '''\n",
    "        \n",
    "        self.w_ = np.zeros(1 + X.shape[1])\n",
    "        self.cost_ = []\n",
    "        for i in range(self.n_iter):\n",
    "            output = self.net_input(X)\n",
    "            errors = (y - output)\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            cost = (errors**2).sum()/2.0\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "        \n",
    "        def net_input(self, X):\n",
    "            '''Calculate net input'''\n",
    "            return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "        \n",
    "        def activation(self, X):\n",
    "            '''Compute linear activation'''\n",
    "            return self.net_input(X)\n",
    "        \n",
    "        def predict(self, X):\n",
    "            '''Return class label after unit step'''\n",
    "            return np.where(self.activation(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of updating the weights after evaluating each individual training sample, as in the perceptron, we calculate the gradient based on the whole training dataset via self.eta * errors.sum() for the zero-weight and via self.eta * X.T.dot( errors) for the weights 1 to where X.T.dot( errors) is a matrix-vector multiplication between our feature matrix and the error vector. Similar to the previous perceptron implementation, we collect the cost values in a list self.cost_ to check if the algorithm converged after training.\n",
    "\n",
    "Raschka, Sebastian (2015-09-23). Python Machine Learning (p. 38). Packt Publishing. Kindle Edition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-79faef7ae86b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0madal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdalineGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAEACAYAAAB4R+XjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naAAYkIIGgAEpaAAY0P8BsFU2rmJ0OiUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cd13358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#In practice, it often requires some experimentation to find a good learning rate for optimal convergence.\n",
    "#So, let's choose two different learning rates and to start with and plot the cost functions versus the \n",
    "#number of epochs to see how well the Adaline implementation learns from the training data.\n",
    "#Raschka, Sebastian (2015-09-23). Python Machine Learning (pp. 38-39). Packt Publishing. Kindle Edition.\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\n",
    "adal = AdalineGD(n_iter=10, eta=0.01).fit(X,y)\n",
    "ax[0].plot(range(1, len(adal.cost_)+1), np.log10(adal.cost_), marker='o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(Sum-squared-error)')\n",
    "ax[0].set_title('Adaline - Learning rate 0.01')\n",
    "ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X,y)\n",
    "ax[1].plot(range(1, len(ada2.cost_)+1), ada2.cost_, marker='o')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Sum-squared-error')\n",
    "ax[1].set_title('Adaline - Learning rate 0.0001')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
